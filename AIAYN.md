# Attention Is All You Need

## 動機[1]

RNN(LSTM、GRU)在當時2017年，是建構先進效能序列模型(NLP：翻譯、分類、QA、摘要)的主要方法，RNN並行問題拖慢速度。

## 回顧[2]

### 取代RNN的方法 --> CNN(卷積)

[Can Active Memory Replace Attention?](https://arxiv.org/pdf/1610.08613.pdf)

[Neural Machine Translation in Linear Time](https://arxiv.org/pdf/1610.10099.pdf)

[Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)

[**卷積的問題**](https://youtu.be/ugWDIIOHtPA?t=192)

### 注意力機制（intra Attention）

## 方法[2]

## 結果[1]

## 貢獻[1]

<!-- 論文的動機、簡述過去相關研究、研究方法、結果、貢獻。 -->