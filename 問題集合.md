# 問題集合

## 1.過去進行中文文字處理強調分詞的重要，但是愈來愈多研究表明分詞並沒有必要。甚至認為不需要排除停用詞，為什麼呢?這牽涉到我們的實驗要分詞嗎?要排除停用詞嗎?

## 2.有人認為西文適用RNN，中文適用CNN，這是甚麼想法?

## 3.以LSTM而言，如果我們希望藉由過去50年數據來預測未來10年的數據，那麼我們是將50年數據(1,50)一起輸入，還是將每一年數據當作一個時間點，分別輸入?另外過去50年的數據，如果我們只依據一個變量做預測，那麼適不適合用LSTM?

## 4.過去RNN, CNN已經有Attention的機制，Google所提出的Attention is all you need這篇與前述的差別，可歸納幾項特點?

## 5. Transformer原論文Encoding用Softmax做激活，而我們用Relu，能否提供有利的實驗支持?

## 6. Transformer原論文Encoding, Decoding在小型的資料集各用六層，而我們只用兩層，能否提供有利的實驗支持?

## 8. Transformer的Add & Norm在做甚麼? Multi-Head有甚麼作用? 從Encoding到Decoding維度有變化嗎?

## 9. RNN沒有用Positional Encoding，為什麼Transformer要用?

## 10 Encoding一開始有(shifted right)，這是甚麼意思?

## 11.請用一個中翻英的短句解釋Masked Multi-Head Attention

## 12.請用前後詩文的例子解釋Masked Multi-Head Attention

## 13. Transformer有甚麼可改善的?
