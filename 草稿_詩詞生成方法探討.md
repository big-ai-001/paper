# 詩詞生成方法探討

<!-- 問小敏 -->

## 摘要

新詩，你可以在捷運的廣告燈箱上看見，也可以在音樂中聽見，是一種廣泛應用在生活之中卻鮮少被發現的文體。且押韻自由，無固定格律，以詩人的個性與感受砌成，有著許多大膽有趣的手法。在現代新詩中用更廣義的定義,將藏頭拓展成藏在任意位子,不管是斜線或圖形都是這篇裡「藏頭」所指的範圍。
在本研究提出一套流程以詞或字做為起頭，續寫為一篇新詩，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法，本研究使用[Jianlin Su et al.](https://arxiv.org/abs/2104.09864)中開源的 ROFORMER 預訓練語言模型並結合[Li Dong et al.](https://arxiv.org/abs/1905.03197)提出的 UniLM 架構進行微調，並在微調過程中加入生成條件，訓練模型依條件改變生成結果，如生成藏頭詩。除了在詩中藏入訊息以外,使用者也可以指定韻腳以及句子長度,達到細微程度去控制模型生成句子的研究。

關鍵字：新詩生成、

## 1.研究動機與目的

新詩，你可以在捷運的廣告燈箱上看見，也可以在音樂中聽見，是一種廣泛應用在生活之中卻鮮少被發現的文體。且押韻自由，無固定格律，以詩人的個性與感受砌成，有著許多大膽有趣的手法，如藏頭詩。
有鑑於深度學習的崛起，許多研究顯示深度學習在詩詞生成方面皆有不錯的效果，且是生成新詩的重點。

本研究最主要的目標是運用深度學習技術訓練出一個新詩生成模型，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法。

## 2.相關研究與文獻探討

### 2.1 ROFORMER

ROFORMER 為 Jianlin Su et al.提出的預訓練語言模型，其相較於 bert 有以下不同之處：

#### 2.1.1 字詞混和

使用字為單位，會使序列變長不確定性變高，導致 Exposure Bias 更高，而使用詞為單位，參數更高，更容易過擬和，且中文的詞是無限的，會更容易出現 OOV(Out Of Vocab)的問題。而 ROFORMER 混合以上兩者，其使用傳統中文字表並加入一定數量(TopN)的中文常用詞，一定程度的緩解字單位模型的長序列問題和 Exposure Bias 問題，並且透過 TopN 詞使其相較純詞單位模型的 vocab 縮小許多，而 OOV 詞可轉以字單位表示，該方法解決了兩者的缺點，並最大程度保留兩者的優點。

#### 2.1.2 RoPE 旋轉位置編碼

旋轉位置編碼是透過絕對位置編碼產生相對位置編碼，其目的為解決模型在 Pre-train 時的訓練語料長度和下游端 Finle-turn 微調所使用的語料長度不等造成的長度泛化問題。

參考以下基本 embedding and transformation->參考 Roformer 2.1 公式 1

$$
q_{m}=f_{q}\left(x_{m},m\right)\\
k_{n}=f_{k}\left(x_{n},n\right)\\
v_{n}=f_{v}\left(x_{n},n\right)\\
$$

參考以下基本 相對位置編碼數學假設-> 參考 Roformer 2.3 公式 5

$$
f_{q}\left(x_{m}\right):=W_{q}x_{m}\\
f_{k}\left(x_{n},n\right):=W_{k}\left(x_{n}+\tilde{p}_{r}^{k}\right)\\
f_{v}\left(x_{n},n\right):=W_{v}\left(x_{m}+\tilde{p}_{r}^{v}\right)\\
$$

5-1 為基礎 linear transformation,5-2 加入新的位置參數 n 則為原向量 加上其位置編碼
再次做 linear transformation 矩陣為 Wk 為 fk,5-3 加入新的 位置參數 n 則為原向量 加上其位置編碼
再次做 linear transformation 矩陣為 Wv 為 fv

首先為了給 qk 添加絕對位置訊息 假設 q,k ->科學思路 公式 1

$$\tilde {q}_m=f\left(q,m\right),\tilde{k}_{n}=f\left(k,n\right)$$

使用內積->科學思路 公式 2

$$\left \langle f\left ( q, m \right ),f \left ( k,n \right ) \right \rangle = g\left ( q,k,m-n \right )$$

使用 複數指數函數 方式求解
首先 先看複數指數 的定義 -><https://zh.wikipedia.org/zh-tw/%E6%AC%A7%E6%8B%89%E5%85%AC%E5%BC%8F>
的 驗證方法 之 泰勒展開 證明複數指數的基本定義
更改複數指數定義為
$$e^{i \theta f\left ( q,m \right )} ,e^{i \theta f\left ( k,n \right )},e^{i \theta g\left ( q,k,m-n \right )}$$

使用 科學空間->求解過程 公式 4-8

$$
f\left ( q,m \right ) = R_{f}\left ( q,m \right ) e^{i\theta f\left ( q,m \right ) }\\
f\left ( k,n \right ) = R_{f}\left ( k,n \right ) e^{i\theta f\left ( k,n \right ) }\\
g\left ( q,k,m-n \right ) = R_{g}\left ( q,k,m-n \right ) e^{i\theta f\left ( q,k,m-n \right ) }
$$

其中 4 式 R 為 單範正交矩陣(orthonormal matrix),m-n 為相對位置
將第四式 第一個 $eq(4.1)*eq(4.2)$ 得到 5.1
將第四式 第一個 $eq(4.1)*eq(4.2)$
已知 $Rf(q,m)Rf(k,n)=g(q,k,m-n)$ 則
$Rf(q,m)e^{i\theta f(q,m)}Rf(k,n)e^{i\theta f(k,n)}=g(q,k,m-n)e^{i\theta g(q,k,m−n)}$ 對比指數 得到 5.2

$$
R_{f}\left ( q,m \right ) R_{f}\left ( k,n \right ) = R_{g}\left ( q,k,m-n \right )\\
\theta _{f}\left ( q,m \right ) - \theta_{f}\left ( k,n \right ) = \theta _{g}\left ( q,k,m-n \right )
$$

第 6 式
由於 $m=n$ ,所以向量內積 $Rf(q,0)Rf(k,0)$ 之夾角為 0 又 $f(q,0)=q,f(k,0)=k$ 內積為 $||q||||k||cos0=||q||||k||$
即

$$Rf(q,m)Rf(k,m)=Rg(q,k,0)=Rf(q,0)Rf(k,0)=∥q∥∥k∥$$

第 8 式 令 $\varphi(m)=m\theta,\varphi(m-1)=m\theta-\theta$ 相減得 $(m-m+1)\theta=\theta$
得到結論 $\varphi(m)=m\theta$

$$\varphi \left( m \right)-\varphi \left( m-1 \right)=\theta_{g} \left( q,k,1 \right) + \theta \left( k \right)-\theta \left( q \right)$$

編碼形式

第 9 式
依據 $Rf(q,m)=||q||,\theta(q)=0$ 可輕易得到第 9 式結果 即 本章節的主題 RoPE

$$f\left(q,m\right)=R_{f}\left(q,m\right)e^{i\theta f(q,m)}=\left\|q\right\|e^{i\left(\theta \left(q \right)+m\theta \right )}=qe^{im\theta }$$

複數 乘法 $\left(a+b_{i}\right)\left(c+d_{i}\right)=ac+ad^{i}+bc_{i}+bd^{i^{2}}=ac-bd+i\left(bc+ad\right)$ 改變位置
幾何意義為旋轉，所以稱之為旋轉位置編碼(RoPE) 由於幾何意義為旋轉 所以可以將 $f(q,m)$ 寫成 旋轉矩陣

$$
f(q,m) =
\begin{pmatrix}
  \cos m\theta & −\sin m\theta \\
  \sin m\theta & \cos m\theta
\end{pmatrix}
\begin{pmatrix}
  q_{0} \\
  q_{1}
\end{pmatrix}
$$

現在對每一個 向量做旋轉 注意此為偶數維度 則 任意向量 q 兩兩一組 配上旋轉矩陣則為 11 式
此矩陣為 單範正交矩陣 (orthonormal matrix)

$$
\underbrace{
\begin{pmatrix}
  \cos m\theta_0 & −\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 & \\
  \sin m\theta_0 &  \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 & \\
  0 & 0 &\cos m\theta_1 & −\sin m\theta_1 & \cdots & 0 & 0 & \\
  0 & 0 &\sin m\theta_1 &  \cos m\theta_1 & \cdots & 0 & 0 & \\
  \vdots& \vdots& \vdots& \vdots& \ddots& \vdots& \vdots \\
  0 & 0 & 0 & 0 & \cdots &\cos m\theta_{\frac{d}{2}-1 } & −\sin m\theta_{\frac{d}{2}-1 }& \\
  0 & 0 & 0 & 0 & \cdots &\sin m\theta_{\frac{d}{2}-1 } &  \cos m\theta_{\frac{d}{2}-1 }& \\
\end{pmatrix}
}_{\mathcal{R}_{m}}
\begin{pmatrix}
  q_0\\
  q_1\\
  q_2\\
  q_3\\
  \vdots\\
  q_{d-2}\\
  q_{d-1}\\
\end{pmatrix}
$$

現在给位置為 m 的向量 q 乘上矩陣 Rm、位置 n 的向量 k 乘上矩陣 Rn，用變換後的 Q,K 序列做 Attention，
那麼 Attention 就自动包含相對位置信息即 第 12 式

$$
(\mathcal{R} _{m}q)\top(\mathcal{R} _{n}k)=q^\top \mathcal{R} ^\top_m\mathcal{R} _nk=q^\top \mathcal{R} _{n−m}k
$$

### 2.2 UniLM

UniLM() 為 microsoft 提出的方法，其運用遮罩使 bert 可以在保持原有雙向語言模型功能下額外提供如 GPT 的單向語言模型功能和 seq2seq 架構功能，下圖為 UniLM 模型架構圖。

![unilm](./unilm.png)

Bidirectional LM (上)：
和 bert 相同，再無須 padding 的情況下，不使用任何遮罩(-1e9)使特定 token 的 softmax 注意力權重歸零，使每個 token 皆能關注整個序列其他 token 的資訊。

Left-to-Right LM (中)：
和 GPT 相同，再無須 padding 的情況下，使用遮罩(-1e9)使 token 右方所有 token 的 softmax 注意力權重歸零，使模型能關注序列左方 token 的資訊依序去生成下一個 token。

Seq-to-Seq LM (下)：
將序列分為兩部分 S1(input)和 S2(output)，S1 和 Bidirectional LM 相同，S1 每個 token 皆能關注 S1 序列其他 token 的資訊。S2 則和 Left-to-Right LM 相同，S2 每個 token 皆只關注左方(含 S1)token 的資訊，並依其生成下一個 token。

### 2.3 platten & control signal

## 3.新詩生成模型訓練

下圖展示了本研究從語料處裡到模型測試的流程。

![詩詞實驗流程圖.png](./詩詞實驗流程圖.png)

圖 n：模型訓練實驗流程

本次實驗可以分為三個主要部分：語料處理、模型訓練、效果測試，依清洗和標註後的 <<華語現代詩歌>> 語料，以 99.8:0.1:0.1 的比例切分為訓練集、驗證集和測試集。

### 3.1 語料來源與處理

本次實驗的所使用的資料集為<<華語現代詩歌與料庫>>，截至 2022-07-08 日該資料庫共蒐集了 3425 位詩人，79700 首詩共 15013758 字。每首詩皆以下結構表示。

> title:[标题]
>
> date:[写作日期, YYYYMMDD/YYYYMM/YYYY/空]
>
> [正文]

而對每筆詩詞資料會將其以'\n'分割為行，並對其進行標註，

每筆訓練資料形式如下所示：

| IN/TRA | 資料形式 |
| ------ | -------- |
| INPUT  | [BOS] Ri-3 [SEP] Ri-2 [SEP] Ri-1 [SLICE] 01 你（隨機選 Ri 的字或詞） [SLICE] 02 好（隨機選 Ri 的字或詞） [SLICE] ㄚ(取 Ri 韻腳) [SLICE] 03(len(Ri)) [EOS] |
| TRAGET | [BOS] Ri [EOS] |

表 N：訓練資料格式表。n=random(1~5)，此範例設 n=3，Ri=你好嗎

### 3.2 模型構建

#### 3.2.1 BERT (Bidirectional Encoder Representations from Transformers)

於 2018 年 10 月，Google AI Language 團隊提出了全新的預訓練語意表示(language representation)模型：BERT，其在當時成為了 11 項自然語言處理任務的 SOTA(State Of The Art)模型，該模型透過雙向(無向)的 Transformers，並使用 MLM(Mask Language Modle)和 NSP(Next Sentence Predict)兩樣 self supervse learning Task 進行訓練，使其擁有取得文本語意嵌入的能力。這兩項任務中，MLM 將輸入序列以一定機率隨機遮蓋，並訓練模型透過上下文訊息將遮蓋字詞還原，而 NSP 則是將兩個句子輸入，並訓練模型判斷這對句子是否連續，從而使模型建構句之間的語意關係。

#### 3.2.2 RoBERTa & ROFORMER

於 2019 年，RoBERTa(Robustly optimized BERT approach)的作者 Liu 等人認為原始的 BERT 是欠擬和(undertrained)的，論文的作者採取了許多優化方法來增進 BERT 的表現，並依此訓練出了 RoBERTa 預訓練語言模型，這些優化方法包含：

- Dynamic Masking(動態遮罩)：相較於原始 BERT 在只在資料預處理時產生進行隨機遮掩，RoBERTa 則是在每個 epoch 開使前將與料重新執行一次隨機遮掩，使同一筆資料在整個訓練過程擁有更多的樣態，讓模型降低過擬和的風險。
- WWM(Whole Word Masking, 全詞遮掩)：原始 BERT 在中文資料的 token 是以字為單位，這造成對 token 隨機遮掩時是以字為單位進行，沒有考慮到該字與前後其他字組合出更複雜的詞義，為解決此問題 RoBERTa 將語料分詞，若隨機遮掩到詞的部分，則會將詞完全遮蓋。
- NSP(Next Sentence Prediction)移除：RoBERTa 作者在經過實驗後選擇將 NSP 任務移除，選擇以增長訓練文本長度的策略將其替代。

於 2021 年，Rofoemer 在 RoBERTa 的原始權重和 vocab 下額外加入新的中文常用詞表和 RoPE 位置編碼進行預訓練，為改進RoBERTa在預訓練時平均序列長度和微調時所使用資料的平均序列長度不等所造成的擬和問題。

### 3.3 模型評測演算法

#### 3.3.1 BLEU(Bilingual Evaluation Understudy)

BLEU 演算法是一種常用於評斷機器文本與目標文本相似度的方法，該演算法基於準確率，將文本以 1~N-gram(N 常設 4)分割計算機器文本與目標文本的準確率並加總，。

$$
BLEU= BP\times exp\left ( \sum_{n=1}^{N} W_n \times \log{P_n} \right )\\
BP=\begin{cases}
  & 1 & lc > lr\\
  & exp(1- \frac{lr}{lc}) & lc\le lr
\end{cases}\\
\text{ lc = 機器文長 }\\
\text{ lr = 參考文長 }
$$

#### 3.3.2 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)

ROUGE 演算法 3.3.5 提到的 BLEU 演算法相似也是一種常用於評斷機器文本與目標文本相似度的方法，該演算法基於召回率，將文本以 1~N-gram(N 常設 4)分割計算機器文本與目標文本的準確率並加總，。

$$
\frac{\sum_{S\in \left \{ ReferemceSummaries \right \} }\sum_{gram_n\in s}Count_{match}\left ( gram_n \right )}{\sum_{S\in \left \{ ReferemceSummaries \right \} }\sum_{gram_n\in s}Count\left ( gram_n \right )}
$$

#### 3.3.3 測試結果

以下為本次訓練之模型評價比較表，

| model    | BLEU     | ROUGE-1  | ROUGE-2  | ROUGE-l  |
| ------   | ------   | ------   | ------   | ------   |
| TRBASE   | 0.072351 | 0.353564 | 0.134306 | 0.341765 |
| ROFORMER | 0.112315 | 0.413476 | 0.170137 | 0.399999 |

## 4.系統展示

![新詩生成器](%E6%96%B0%E8%A9%A9%E7%94%9F%E6%88%90%E5%99%A8.jpeg)

## 5.結論

以下只是概述尚未完成：

- 提供一訓練方法使模型可以依條件生成句子(目前無人實現:指定位置生成不定長文本，台大那篇只能指定位置生成一字 or 一詞，和注音韻腳)

- 口吃(震盪)

- 一步錯，步步錯

- 無法生成圖象詩

設 str=(x1,x2,...xn)，對 str 內的每個元素使用 bert 嵌入為向量，使用 BERT-NSP 將最小可視單位合併為句單位(合併後的文本向量為原文本向量的和)，計算合併後每個句單位元素之向量夾角將小於閥值的單位合併為段落單位(合併後的文本向量為原文本向量的和)。
相較於原方法不須在重新使用 BERT 將合併後文本重新嵌入為向量，而是重復使用最小可視單位文本的嵌入向量透過向量加法組合為句單位和段落單位，以節省計算時間。
為何符合 Flyweight Pattern：每個段落單位或句單位皆是由可共用和復用的最小可視單位構成。
為何符合 Composite Pattern：只實作出每個最小可視單位的文本嵌入，每個段落單位或句單位皆是由最小可視單位兜起來的。

