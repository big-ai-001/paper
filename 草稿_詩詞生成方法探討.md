# 詩詞生成方法探討

## 摘要

  本研究提出一套流程以詞或字做為起頭，續寫為一篇新詩，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法，本研究使用[Jianlin Su et al.](https://arxiv.org/abs/2104.09864)中開源的ROFORMER預訓練語言模型並結合[Li Dong et al.](https://arxiv.org/abs/1905.03197)提出的UniLM架構進行微調，並在微調過程中加入生成條件，訓練模型依條件改變生成結果。

關鍵字：新詩生成

## 1.研究動機與目的

新詩，你可以在捷運的廣告燈箱上看見，也可以在音樂中聽見，是一種廣泛應用在生活之中卻鮮少被發現的文體。不以任何形式為形式，以詩人的個性與感受砌成，有著許多大膽有趣的手法。

本研究最主要的目標是運用深度學習技術訓練出一個新詩生成模型，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法。

## 2.相關研究與文獻探討

### 2.1 ROFORMER

ROFORMER 為Jianlin Su et al.提出的預訓練語言模型，其相較於bert有以下不同之處：

一：字詞混和

使用字為單位，會使序列變長不確定性變高，導致Exposure Bias更高，而使用詞為單位，參數更高，更容易過擬和，且中文的詞是無限的，會更容易出現OOV(Out Of Vocab)的問題。而ROFORMER混合以上兩者，其使用傳統中文字表並加入一定數量(TopN)的中文常用詞，一定程度的緩解字單位模型的長序列問題和Exposure Bias問題，並且透過TopN詞使其相較純詞單位模型的vocab縮小許多，而OOV詞可轉以字單位表示，該方法解決了兩者的缺點，並最大程度保留兩者的優點。

二：RoPE 旋轉位置編碼

位置編碼的形式也一定程度的影響了預訓練語言模型的在不同長度語料的泛化能力，

### 2.2 UniLM

UniLM() 為 microsoft 提出的方法，其運用遮罩使bert可以在保持原有雙向語言模型功能下額外提供如GPT的單向語言模型功能和seq2seq架構功能，下圖為UniLM模型架構圖。

![unilm](./unilm.png)

Bidirectional LM (上)：
和bert相同，再無須padding的情況下，不使用任何遮罩(-1e9)使特定token的softmax注意力權重歸零，使每個token皆能關注整個序列其他token的資訊。

Left-to-Right LM (中)：
和GPT相同，再無須padding的情況下，使用遮罩(-1e9)使token右方所有token的softmax注意力權重歸零，使模型能關注序列左方token的資訊依序去生成下一個token。

Seq-to-Seq LM (下)：
將序列分為兩部分S1(input)和S2(output)，S1和Bidirectional LM相同，S1每個token皆能關注S1序列其他token的資訊。S2則和Left-to-Right LM相同，S2每個token皆只關注左方(含S1)token的資訊，並依其生成下一個token。

<!-- ### 2.3 platten & control signal -->

## 3.新詩生成模型訓練

下圖展示了本研究從語料處裡到模型測試的流程。

![詩詞實驗流程圖.png](./詩詞實驗流程圖.png)

圖n：模型訓練實驗流程

本次實驗可以分為三個主要部分：語料處理、模型訓練、效果測試，依清洗和標註後的 <<華語現代詩歌>> 語料，以 99.8:0.1:0.1 的比例切分為訓練集、驗證集和測試集。

### 3.1 語料來源與處理

本次實驗的所使用的資料集為<<華語現代詩歌與料庫>>，截至2022-07-08日該資料庫共蒐集了3425位詩人，79700首詩共15013758字。每首詩皆以下結構表示。

> title:[标题]
>
> date:[写作日期, YYYYMMDD/YYYYMM/YYYY/空]
>
> [正文]

而對每筆詩詞資料會將其以'\n'分割為行，並對其進行標註，

### 3.2 模型構建

### 3.3 模型驗證

## 4.系統展示
