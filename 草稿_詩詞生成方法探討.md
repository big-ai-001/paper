# 詩詞生成方法探討
<!-- 問小敏 -->

## 摘要

  本研究提出一套流程以詞或字做為起頭，續寫為一篇新詩，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法，本研究使用[Jianlin Su et al.](https://arxiv.org/abs/2104.09864)中開源的ROFORMER預訓練語言模型並結合[Li Dong et al.](https://arxiv.org/abs/1905.03197)提出的UniLM架構進行微調，並在微調過程中加入生成條件，訓練模型依條件改變生成結果。

關鍵字：新詩生成、

## 1.研究動機與目的

新詩，你可以在捷運的廣告燈箱上看見，也可以在音樂中聽見，是一種廣泛應用在生活之中卻鮮少被發現的文體。不以任何形式為形式，以詩人的個性與感受砌成，有著許多大膽有趣的手法。

本研究最主要的目標是運用深度學習技術訓練出一個新詩生成模型，並且可以指定每一篇詩的韻腳、長度和指定位置使用什麼字以提供一些特殊的修辭方法。

## 2.相關研究與文獻探討

### 2.1 ROFORMER

ROFORMER 為Jianlin Su et al.提出的預訓練語言模型，其相較於bert有以下不同之處：

#### 2.1.1 字詞混和

使用字為單位，會使序列變長不確定性變高，導致Exposure Bias更高，而使用詞為單位，參數更高，更容易過擬和，且中文的詞是無限的，會更容易出現OOV(Out Of Vocab)的問題。而ROFORMER混合以上兩者，其使用傳統中文字表並加入一定數量(TopN)的中文常用詞，一定程度的緩解字單位模型的長序列問題和Exposure Bias問題，並且透過TopN詞使其相較純詞單位模型的vocab縮小許多，而OOV詞可轉以字單位表示，該方法解決了兩者的缺點，並最大程度保留兩者的優點。

#### 2.1.2 RoPE 旋轉位置編碼

旋轉位置編碼是透過絕對位置編碼產生相對位置編碼

參考以下基本 embedding and transformation->參考Roformer 2.1 公式1
$$
q_{m}=f_{q}\left(x_{m},m\right)\\
k_{n}=f_{k}\left(x_{n},n\right)\\
v_{n}=f_{v}\left(x_{n},n\right)\\
$$
參考以下基本 相對位置編碼數學假設-> 參考Roformer 2.3 公式5
$$
f_{q}\left(x_{m}\right):=W_{q}x_{m}\\
f_{k}\left(x_{n},n\right):=W_{k}\left(x_{n}+\tilde{p}_{r}^{k}\right)\\
f_{v}\left(x_{n},n\right):=W_{v}\left(x_{m}+\tilde{p}_{r}^{v}\right)\\
$$
5-1 為基礎 linear transformation,5-2 加入新的位置參數n 則為原向量 加上其位置編碼
再次做linear transformation 矩陣為Wk 為fk,5-3 加入新的 位置參數n 則為原向量 加上其位置編碼
再次做linear transformation 矩陣為Wv 為fv

首先為了給qk添加絕對位置訊息 假設 q,k ->科學思路 公式1

$\tilde{q}_m=f\left(q,m\right),\tilde{k}_{n}=f\left(k,n\right)$

使用內積->科學思路 公式2

$\left \langle f\left ( q, m \right ),f \left ( k,n \right ) \right \rangle = g\left ( q,k,m-n \right ) $

使用 複數指數函數 方式求解
首先 先看複數指數 的定義 -><https://zh.wikipedia.org/zh-tw/%E6%AC%A7%E6%8B%89%E5%85%AC%E5%BC%8F>
的 驗證方法 之 泰勒展開 證明複數指數的基本定義
更改複數指數定義為
$e^{i} \theta f\left ( q,m \right ) ,e^{i} \theta f\left ( k,n \right ),e^{i} \theta g\left ( q,k,m-n \right )$

使用 科學空間->求解過程 公式4-8
$$
f\left ( q,m \right ) = R_{f}\left ( q,m \right ) e^{i\theta f\left ( q,m \right ) }\\
f\left ( k,n \right ) = R_{f}\left ( k,n \right ) e^{i\theta f\left ( k,n \right ) }\\
g\left ( q,k,m-n \right ) = R_{g}\left ( q,k,m-n \right ) e^{i\theta f\left ( q,k,m-n \right ) }
$$
其中4式 R為 單範正交矩陣(orthonormal matrix),m-n為相對位置
將第四式 第一個 $eq(4.1)*eq(4.2)$ 得到5.1
將第四式 第一個 $eq(4.1)*eq(4.2)$
已知 $Rf(q,m)Rf(k,n)=g(q,k,m-n)$ 則
$Rf(q,m)e^i\theta f(q,m)Rf(k,n)e^i\theta f(k,n)=g(q,k,m-n)e^i\theta f(q,m)i\theta g(q,k,m−n)$ 對比指數 得到5.2
$$
R_{f}\left ( q,m \right ) R_{f}\left ( k,n \right ) = R_{g}\left ( q,k,m-n \right )\\
\theta _{f}\left ( q,m \right ) - \theta_{f}\left ( k,n \right ) = \theta _{g}\left ( q,k,m-n \right )
$$
第6式
由於 $m=n$ ,所以向量內積 $Rf(q,0)Rf(k,0)$ 之夾角為0又 $f(q,0)=q,f(k,0)=k$ 內積為 $||q||||k||cos0=||q||||k||$
即

$Rf(q,m)Rf(k,m)=Rg(q,k,0)=Rf(q,0)Rf(k,0)=∥q∥∥k∥$

第8式 令 $\varphi(m)=m\theta,\varphi(m-1)=m\theta-\theta$ 相減得 $(m-m+1)\theta=\theta$
得到結論 $\varphi(m)=m\theta$

$\varphi \left( m \right)-\varphi \left( m-1 \right)=\theta_{g} \left( q,k,1 \right) + \theta \left( k \right)-\theta \left( q \right)$

編碼形式

第9式
依據 $Rf(q,m)=||q||,\theta(q)=0$ 可輕易得到第9式結果 即 本章節的主題 RoPE

$f\left(q,m\right)=R_{f}\left(q,m\right)e^{i\theta f(q,m)}=\left\|q\right\|e^{i\left(\theta \left(q \right)+m\theta \right )}=qe^{im\theta }$

複數 乘法 $\left(a+b_{i}\right)\left(c+d_{i}\right)=ac+ad^{i}+bc_{i}+bd^{i^{2}}=ac-bd+i\left(bc+ad\right)$ 改變位置
幾何意義為旋轉，所以稱之為旋轉位置編碼(RoPE) 由於幾何意義為旋轉 所以可以將 $f(q,m)$ 寫成 旋轉矩陣
$$
f(q,m) =
\begin{pmatrix}  
  \cos m\theta & −\sin m\theta \\  
  \sin m\theta & \cos m\theta  
\end{pmatrix}
\begin{pmatrix}  
  q_{0} \\  
  q_{1}  
\end{pmatrix}
$$

現在對每一個 向量做旋轉 注意此為偶數維度 則 任意向量q 兩兩一組 配上旋轉矩陣則為11式
此矩陣為 單範正交矩陣 (orthonormal matrix)

$$
\underbrace{
\begin{pmatrix}  
  \cos m\theta_0 & −\sin m\theta_0 & 0 & 0 & \cdots & 0 & 0 & \\  
  \sin m\theta_0 &  \cos m\theta_0 & 0 & 0 & \cdots & 0 & 0 & \\  
  0 & 0 &\cos m\theta_1 & −\sin m\theta_1 & \cdots & 0 & 0 & \\  
  0 & 0 &\sin m\theta_1 &  \cos m\theta_1 & \cdots & 0 & 0 & \\  
  \vdots& \vdots& \vdots& \vdots& \ddots& \vdots& \vdots \\  
  0 & 0 & 0 & 0 & \cdots &\cos m\theta_{\frac{d}{2}-1 } & −\sin m\theta_{\frac{d}{2}-1 }& \\  
  0 & 0 & 0 & 0 & \cdots &\sin m\theta_{\frac{d}{2}-1 } &  \cos m\theta_{\frac{d}{2}-1 }& \\
\end{pmatrix} 
}_{\mathcal{R}_{m}} 
\begin{pmatrix}  
  q_0\\
  q_1\\
  q_2\\
  q_3\\
  \vdots\\
  q_{d-2}\\
  q_{d-1}\\
\end{pmatrix} 


$$

現在给位置為m的向量q乘上矩陣Rm、位置n的向量k乘上矩陣Rn，用變換後的Q,K序列做Attention，
那麼Attention就自动包含相對位置信息即 第12式

$$
(\mathcal{R} _{m}q)\top(\mathcal{R} _{n}k)=q^\top \mathcal{R} ^\top_m\mathcal{R} _nk=q^\top \mathcal{R} _{n−m}k
$$

### 2.2 UniLM

UniLM() 為 microsoft 提出的方法，其運用遮罩使bert可以在保持原有雙向語言模型功能下額外提供如GPT的單向語言模型功能和seq2seq架構功能，下圖為UniLM模型架構圖。

![unilm](./unilm.png)

Bidirectional LM (上)：
和bert相同，再無須padding的情況下，不使用任何遮罩(-1e9)使特定token的softmax注意力權重歸零，使每個token皆能關注整個序列其他token的資訊。

Left-to-Right LM (中)：
和GPT相同，再無須padding的情況下，使用遮罩(-1e9)使token右方所有token的softmax注意力權重歸零，使模型能關注序列左方token的資訊依序去生成下一個token。

Seq-to-Seq LM (下)：
將序列分為兩部分S1(input)和S2(output)，S1和Bidirectional LM相同，S1每個token皆能關注S1序列其他token的資訊。S2則和Left-to-Right LM相同，S2每個token皆只關注左方(含S1)token的資訊，並依其生成下一個token。

### 2.3 platten & control signal

## 3.新詩生成模型訓練

下圖展示了本研究從語料處裡到模型測試的流程。

![詩詞實驗流程圖.png](./詩詞實驗流程圖.png)

圖n：模型訓練實驗流程

本次實驗可以分為三個主要部分：語料處理、模型訓練、效果測試，依清洗和標註後的 <<華語現代詩歌>> 語料，以 99.8:0.1:0.1 的比例切分為訓練集、驗證集和測試集。

### 3.1 語料來源與處理

本次實驗的所使用的資料集為<<華語現代詩歌與料庫>>，截至2022-07-08日該資料庫共蒐集了3425位詩人，79700首詩共15013758字。每首詩皆以下結構表示。

> title:[标题]
>
> date:[写作日期, YYYYMMDD/YYYYMM/YYYY/空]
>
> [正文]

而對每筆詩詞資料會將其以'\n'分割為行，並對其進行標註，

每筆訓練資料形式如下所示：

|IN/TRA|資料形式|
|---   |---     |
|INPUT |[BOS] Ri-3 [SEP] Ri-2 [SEP] Ri-1 [SLICE] 01你（隨機選Ri的字或詞） [SLICE] 02好（隨機選Ri的字或詞） [SLICE] ㄚ(取Ri韻腳) [SLICE] 03(len(Ri)) [EOS]|
|TRAGET|[BOS] Ri [EOS]|

表N：訓練資料格式表。n=random(1~5)，此範例設n=3，Ri=你好嗎

### 3.2 模型構建

#### 3.2.1 BERT (Bidirectional Encoder Representations from Transformers)

於2018年10月，Google AI Language團隊提出了全新的預訓練語意表示(language representation)模型：BERT，其在當時成為了11項自然語言處理任務的SOTA(State Of The Art)模型，該模型透過雙向(無向)的Transformers，並使用MLM(Mask Language Modle)和NSP(Next Sentence Predict)兩樣unsupervse learning Task進行訓練，使其擁有取得文本語意嵌入的能力。這兩項任務中，MLM將輸入序列以一定機率隨機遮蓋，並訓練模型透過上下文訊息將遮蓋字詞還原，而NSP則是將兩個句子輸入，並訓練模型判斷這對句子是否連續，從而使模型建構句之間的語意關係。

#### 3.2.2 ROFORMER

### 3.3 模型評測演算法

#### 3.3.5 BLEU(Bilingual Evaluation Understudy)

BLEU演算法是一種常用於評斷機器文本與目標文本相似度的方法，該演算法基於準確率，將文本以1~N-gram(N常設4)分割計算機器文本與目標文本的準確率並加總，

#### 3.3.6 ROUGE(Recall-Oriented Understudy for Gisting Evaluation)

## 4.系統展示

![新詩生成器](%E6%96%B0%E8%A9%A9%E7%94%9F%E6%88%90%E5%99%A8.jpeg)

## 5.結論

以下只是概述尚未完成：

* 提供一訓練方法使模型可以依條件生成句子(目前無人實現:指定位置生成不定長文本，台大那篇只能指定位置生成一字or一詞，和注音韻腳)

* 口吃(震盪)

* 一步錯，步步錯

* 無法象形詩
